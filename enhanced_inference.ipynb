{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Enhanced Inference \n", "\n", "`autogen.OpenAIWrapper` provides LLM inference for *openai>=1*.\n", "`autogen.Completion` is drop-in replacement of `openai.Completion` and `openai.ChatCompletion` for enhanced LLM inference using *openai<1* .\n", "\n", "## Tune Inference Parameter(for openai<1)\n", "1. Choices to optimize\n", "    the cost of using foundation models for text generation is typically measured in terms of the number of tokens in the input and output combined.\n", "    from the perspective of an application builder using foundation models the use case is to maximize the utility of the generated text under an inference budget constraints \n", "    this can be achieved by optimizing the hyperparameters of the inference which can signficantly affect both the utility and the cost of the generated text.\n", "\n", "The tunable hyperparameters include:\n", "1. model - required input specifying the model ID to use\n", "2. prompt/message - input prompt/message to the model which provides the context for the text generation task \n", "3. max_tokens - maximum number of tokens to generate in the output\n", "4. temperature - a value between 0 and 1 that controls the randomness of the text \n", "5. top_p - a value between 0 and 1 that controls the sampling probability mass for each token generation\n", "6. n - number of response to generate for a given prompt\n", "7. stop - list of string that when encountered in the generated text, will cause the generation to stop.\n", "8. presence_penalty , frequency_penalty - values that control the relative importance of the presence and frequency of certain words or phrares in the generated text\n", "9. best_of - number of response to generate server-side when selecting the best response for a given prompt \n", "\n", "The cost and utility of text generation are intertwined with the joint effect of these hyperparameters. There are also complex interactions among subsets of the hyperparameters.\n", "\n", "Tuning can be performed with the following information:\n", "1. Validation data :\n", "\n", "    collect a diverse set of instances and can be stored in an iterable dict.\n", "\n", "2. Evaluation function :\n", "\n", "    evaluation function should take a list of responses, and other keyword arguments corresponding to the keys in each validation data instance as input, and output a dict of metrics. \n", "\n", "    `autogen.code_utils` and `autogen.math_utils` offer some example evaluation functions for code generation and math problem solving.\n", "\n", "3. Metric to optimize\n", "\n", "    metric to optimize is usually an aggregated metric over all the tuning data instances\n", "\n", "4. Search space \n", "\n", "    i. **Model :** can be fixed string or multiple choices defined using `flaml.tune.choice`\n", "\n", "    ii. **Prompt/messages :** prompt is a template for the model's input, which could be a single string or a list of strings. Messages are the model's instructions or context, which could be either a list of dictionaries or lists of lists.\n", "\n", "    iii. **max_tokens,n ,best_of :** hyperparameters control how much text the model generates.\n", "    can either be set to fixed values or searched within a range using `flaml.tune.randint`, `flaml.tune.qrandint`\n", "\n", "    iv. **stop :** specifies where the text generation should stop\n", "    it can be a string, a list of strings or `None`\n", "\n", "    v. **temperature ot top_p :** settings control the randomness of the text. You can choose either `temperature` or `top_p`, but not both at the same time. \n", "    they can be set as constants or searched within a range using methods like `flaml.tune.uniform` or `flaml.tune.loguniform`. \n", "    The default range for both is between 0 and 1.\n", "\n", "    vi. **presence_penalty, frequency_penalty :**  control penalties applied to repeated phrases. \n", "    By default, these are not tuned, but you can specify them as constants or search for them using methods like `flaml.tune.uniform`.\n", "\n", "5. Budget: inference budget refers to the average inference cost per data instance. \n", "The optimization budget refers to the total budget allowed in the tuning process. \n", "Both are measured by dollars and follow the price per 1000 tokens.\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Performance tuning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import autogen\n", "\n", "config, analysis = autogen.Completion.tune(\n", "    data=tune_data,\n", "    metric=\"success\",\n", "    mode=\"max\",\n", "    eval_func=eval_func,\n", "    inference_budget=0.05,\n", "    optimization_budget=3,\n", "    num_samples=-1,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### API unification\n", "\n", "`autogen.OpenAIWrapper.create()` can be used to create completions for both chat and non-chat models and both OpenAI API and Azure OpenAI API\n", "\n", "For local LLMs one can spin up an endpoint using a package like FastChat and then use same API to send a request. for custom model clients one can register the client with `autogen.OpenAIWrapper.register_model_client` and then use the same API to send the request.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from autogen import OpenAIWrapper\n", "# OpenAI endpoint\n", "client = OpenAIWrapper()\n", "# ChatCompletion\n", "response = client.create(messages=[{\"role\": \"user\", \"content\": \"2+2=\"}], model=\"gpt-3.5-turbo\")\n", "# extract the response text\n", "print(client.extract_text_or_completion_object(response))\n", "# get cost of this completion\n", "print(response.cost)\n", "# Azure OpenAI endpoint\n", "client = OpenAIWrapper(api_key=..., base_url=..., api_version=..., api_type=\"azure\")\n", "# Completion\n", "response = client.create(prompt=\"2+2=\", model=\"gpt-3.5-turbo-instruct\")\n", "# extract the response text\n", "print(client.extract_text_or_completion_object(response))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Usage Summary\n", "\n", "`OpenAIWrapper` from autogen tracks token counts and costs of your API calls.\n", "use the `create()` method to initiate requests and `print_usage_summary` to retrive detailed usage reports, including total cost and token usage for both cached and actualed requests.\n", "\n", "`mode=[\"actual\",\"total\"]` default: print usage summary for all completions and non-caching completions\n", "\n", "`mode=\"actual\"` : only print non-cached usage\n", "\n", "`mode=\"total\"` : only print all usage (including cache)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "autogen", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.7"}}, "nbformat": 4, "nbformat_minor": 2}